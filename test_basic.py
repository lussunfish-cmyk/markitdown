#!/usr/bin/env python
"""
ê¸°ë³¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- config ì„¤ì • í™•ì¸
- ollama_client ì—°ê²° í…ŒìŠ¤íŠ¸
- API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
"""

import sys
import json
import requests
from pathlib import Path

# ìƒ‰ìƒ ì •ì˜
GREEN = '\033[92m'
RED = '\033[91m'
YELLOW = '\033[93m'
BLUE = '\033[94m'
RESET = '\033[0m'

def print_header(text):
    print(f"\n{BLUE}{'='*60}")
    print(f"  {text}")
    print(f"{'='*60}{RESET}\n")

def print_success(text):
    print(f"{GREEN}âœ… {text}{RESET}")

def print_error(text):
    print(f"{RED}âŒ {text}{RESET}")

def print_info(text):
    print(f"{YELLOW}â„¹ï¸  {text}{RESET}")

def test_config_import():
    """config ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸"""
    print_header("Step 1: Config ëª¨ë“ˆ ì„í¬íŠ¸")
    try:
        from app.config import config
        print_success("config.py ì„í¬íŠ¸ ì„±ê³µ")
        
        print_info(f"Ollama Base URL: {config.OLLAMA.BASE_URL}")
        print_info(f"Embedding Model: {config.OLLAMA.EMBEDDING_MODEL}")
        print_info(f"LLM Model: {config.OLLAMA.LLM_MODEL}")
        print_info(f"Vector Store Type: {config.VECTOR_STORE.STORE_TYPE}")
        print_info(f"Vector Embedding Dim: {config.VECTOR_STORE.EMBEDDING_DIM}")
        print_info(f"Chunk Size: {config.CHUNKING.CHUNK_SIZE}")
        print_info(f"RAG Top K: {config.RAG.TOP_K}")
        
        return True
    except Exception as e:
        print_error(f"config ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_schemas_import():
    """schemas ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸"""
    print_header("Step 2: Schemas ëª¨ë“ˆ ì„í¬íŠ¸")
    try:
        from app.schemas import (
            EmbeddingRequest, EmbeddingResponse,
            RAGRequest, RAGResponse,
            IndexRequest, IndexResponse
        )
        print_success("schemas.py ì„í¬íŠ¸ ì„±ê³µ")
        print_info("ëª¨ë“  ì£¼ìš” ìŠ¤í‚¤ë§ˆ í´ë˜ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        return True
    except Exception as e:
        print_error(f"schemas ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_ollama_client():
    """OllamaClient ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print_header("Step 3: Ollama í´ë¼ì´ì–¸íŠ¸ ì—°ê²°")
    try:
        from app.ollama_client import OllamaClient
        
        print_info("OllamaClient ì´ˆê¸°í™” ì¤‘...")
        client = OllamaClient()
        print_success("Ollama ì„œë²„ ì—°ê²° ì„±ê³µ")
        
        # ëª¨ë¸ í™•ì¸
        models = client.list_models()
        print_info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {models}")
        
        # í•„ìˆ˜ ëª¨ë¸ í™•ì¸
        embedding_available = client.check_model_available(client.embedding_model)
        llm_available = client.check_model_available(client.llm_model)
        
        if embedding_available:
            print_success(f"Embedding ëª¨ë¸ '{client.embedding_model}' ì‚¬ìš© ê°€ëŠ¥")
        else:
            print_error(f"Embedding ëª¨ë¸ '{client.embedding_model}' ë¯¸ì„¤ì¹˜")
            
        if llm_available:
            print_success(f"LLM ëª¨ë¸ '{client.llm_model}' ì‚¬ìš© ê°€ëŠ¥")
        else:
            print_error(f"LLM ëª¨ë¸ '{client.llm_model}' ë¯¸ì„¤ì¹˜")
        
        return embedding_available and llm_available
    except Exception as e:
        print_error(f"Ollama í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_api_health():
    """API í—¬ìŠ¤ ì²´í¬"""
    print_header("Step 4: API í—¬ìŠ¤ ì²´í¬")
    try:
        response = requests.get("http://localhost:8000/health", timeout=5)
        response.raise_for_status()
        data = response.json()
        print_success("API í—¬ìŠ¤ ì²´í¬ ì‘ë‹µ ìˆ˜ì‹ ")
        print_info(f"ì‘ë‹µ: {json.dumps(data, indent=2)}")
        return True
    except Exception as e:
        print_error(f"API í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
        return False

def test_supported_formats():
    """ì§€ì› íŒŒì¼ í˜•ì‹ ì¡°íšŒ"""
    print_header("Step 5: ì§€ì› íŒŒì¼ í˜•ì‹ ì¡°íšŒ")
    try:
        response = requests.get("http://localhost:8000/supported-formats", timeout=5)
        response.raise_for_status()
        data = response.json()
        print_success("ì§€ì› íŒŒì¼ í˜•ì‹ ì¡°íšŒ ì„±ê³µ")
        print_info(f"ì§€ì› í˜•ì‹ ìˆ˜: {data['count']}ê°œ")
        print_info(f"ì§€ì› í˜•ì‹: {', '.join(data['formats'][:10])}...")
        return True
    except Exception as e:
        print_error(f"ì§€ì› íŒŒì¼ í˜•ì‹ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return False

def test_embedding():
    """ì„ë² ë”© í…ŒìŠ¤íŠ¸"""
    print_header("Step 6: ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸")
    try:
        from app.ollama_client import OllamaClient
        
        client = OllamaClient()
        test_text = "ì•ˆë…•í•˜ì„¸ìš”. ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
        
        print_info(f"í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸: '{test_text}'")
        print_info("ì„ë² ë”© ìƒì„± ì¤‘...")
        
        embedding = client.embed(test_text)
        
        print_success("ì„ë² ë”© ìƒì„± ì„±ê³µ")
        print_info(f"ì„ë² ë”© ì°¨ì›: {len(embedding)}")
        print_info(f"ì„ë² ë”© ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): {embedding[:5]}")
        
        return len(embedding) > 0
    except Exception as e:
        print_error(f"ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_ollama_generate():
    """Ollama í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    print_header("Step 7: LLM í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    try:
        from app.ollama_client import OllamaClient
        
        client = OllamaClient()
        prompt = "í•œêµ­ì˜ ìˆ˜ë„ëŠ”?"
        
        print_info(f"í”„ë¡¬í”„íŠ¸: '{prompt}'")
        print_info("ì‘ë‹µ ìƒì„± ì¤‘...")
        
        response = client.generate(prompt, temperature=0.3, num_predict=50)
        
        print_success("í…ìŠ¤íŠ¸ ìƒì„± ì„±ê³µ")
        print_info(f"ì‘ë‹µ: {response}")
        
        return len(response) > 0
    except Exception as e:
        print_error(f"í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    print(f"\n{BLUE}{'='*60}")
    print(f"  MarkItDown RAG ê¸°ë³¸ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print(f"{'='*60}{RESET}\n")
    
    results = []
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results.append(("Config Import", test_config_import()))
    results.append(("Schemas Import", test_schemas_import()))
    results.append(("Ollama Client", test_ollama_client()))
    results.append(("API Health Check", test_api_health()))
    results.append(("Supported Formats", test_supported_formats()))
    results.append(("Embedding Test", test_embedding()))
    results.append(("LLM Generate Test", test_ollama_generate()))
    
    # ê²°ê³¼ ìš”ì•½
    print_header("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = f"{GREEN}âœ… PASS{RESET}" if result else f"{RED}âŒ FAIL{RESET}"
        print(f"{test_name:<30} {status}")
    
    print(f"\nì´ í…ŒìŠ¤íŠ¸: {total}ê°œ")
    print(f"{GREEN}ì„±ê³µ: {passed}ê°œ{RESET}")
    print(f"{RED}ì‹¤íŒ¨: {total - passed}ê°œ{RESET}")
    
    if passed == total:
        print(f"\n{GREEN}ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!{RESET}")
        return 0
    else:
        print(f"\n{RED}âš ï¸  ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨{RESET}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
