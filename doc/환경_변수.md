# 환경 변수 설정 가이드 (Python 네이티브)

MarkItDown RAG 시스템의 설정은 환경 변수로 오버라이드할 수 있습니다.

## 빠른 시작

```bash
cp .env.example .env
```

권장 기본값(네이티브 실행):

```dotenv
LLM_BACKEND_TYPE=lmstudio
LMSTUDIO_BASE_URL=http://localhost:1234/api
LMSTUDIO_LLM_MODEL=qwen3-32b
LMSTUDIO_NO_THINK_ENABLED=true
LMSTUDIO_NO_THINK_SUFFIX=/no_think
LMSTUDIO_NO_THINK_EXTRA_SUFFIX=/nothinking

LMSTUDIO_EMBEDDING_SERVICE_URL=http://localhost:8001
LMSTUDIO_EMBEDDING_MODEL=mxbai-embed-large-v1

MARKITDOWN_INPUT_DIR=./input
MARKITDOWN_OUTPUT_DIR=./output
DOCUMENT_DIR=./output
VECTOR_STORE_DIR=./vector_store
INDEX_STATE_FILE=./vector_store/index_state.json
BATCH_STATE_DIR=./batch_state
TIKTOKEN_CACHE_DIR=./tiktoken_cache
```

---

## 1) LLM/임베딩 설정

| 환경 변수 | 기본값 | 설명 |
|---|---|---|
| `LLM_BACKEND_TYPE` | `lmstudio` | 백엔드 선택 (`lmstudio`/`ollama`) |
| `LMSTUDIO_BASE_URL` | `http://localhost:1234/api` | LM Studio(OpenAI 호환) 서버 URL |
| `LMSTUDIO_LLM_MODEL` | `qwen3-32b` | LM Studio에 로드된 LLM 모델명 |
| `LMSTUDIO_EMBEDDING_SERVICE_URL` | `http://localhost:8001` | 별도 임베딩 서버 URL |
| `LMSTUDIO_EMBEDDING_MODEL` | `mxbai-embed-large-v1` | 임베딩 요청 모델명 |
| `LMSTUDIO_TIMEOUT` | `300` | 요청 타임아웃(초) |
| `LMSTUDIO_MAX_RETRIES` | `3` | 최대 재시도 횟수 |
| `LMSTUDIO_RETRY_DELAY` | `1.0` | 재시도 지연(초) |
| `LMSTUDIO_NO_THINK_ENABLED` | `true` | LM Studio 호출 시 thinking 억제 규칙 적용 여부 |
| `LMSTUDIO_NO_THINK_SUFFIX` | `/no_think` | user prompt 끝에 자동 추가되는 키워드 |
| `LMSTUDIO_NO_THINK_EXTRA_SUFFIX` | `/nothinking` | 추가 억제 키워드(선택) |
| `LMSTUDIO_NO_THINK_SYSTEM_PROMPT` | 내장 기본값 | 요청 앞에 붙는 thinking 억제 지시문 |

참고: 임베딩 서버는 `/v1/models`, `/v1/embeddings`를 제공해야 합니다.

---

## 2) 저장소/경로 설정

| 환경 변수 | 기본값 | 설명 |
|---|---|---|
| `MARKITDOWN_INPUT_DIR` | `./input` | 입력 파일 디렉토리 |
| `MARKITDOWN_OUTPUT_DIR` | `./output` | 변환 결과 디렉토리 |
| `DOCUMENT_DIR` | `./output` | 인덱싱 대상 문서 디렉토리 |
| `VECTOR_STORE_DIR` | `./vector_store` | 벡터 저장소 디렉토리 |
| `INDEX_STATE_FILE` | `./vector_store/index_state.json` | 인덱싱 상태 파일 |
| `BATCH_STATE_DIR` | `./batch_state` | 배치 상태 디렉토리 |
| `TIKTOKEN_CACHE_DIR` | `./tiktoken_cache` | 토크나이저 캐시 |

---

## 3) 검색/RAG 설정

| 환경 변수 | 기본값 | 설명 |
|---|---|---|
| `RAG_TOP_K` | `5` | 검색 문서 수 |
| `RAG_SIMILARITY_THRESHOLD` | `0.5` | 유사도 임계값 |
| `RAG_TEMPERATURE` | `0.3` | 생성 온도 |
| `RAG_TOP_P` | `0.9` | Top-p |
| `RAG_MAX_TOKENS` | `2048` | 최대 생성 토큰 |
| `RAG_MAX_CONTEXT_LENGTH` | `8192` | 최대 컨텍스트 길이 |
| `RAG_ENABLE_QUERY_REWRITING` | `true` | 쿼리 재작성 여부 |
| `RETRIEVER_HYBRID_ALPHA` | `0.5` | 하이브리드 검색 가중치 |
| `RETRIEVER_USE_RERANKER` | `true` | 리랭커 사용 여부 |
| `RETRIEVER_RERANKER_MODEL` | `BAAI/bge-reranker-v2-m3` | 리랭커 모델 |

---

## 4) 변환/청킹/API 설정

| 환경 변수 | 기본값 | 설명 |
|---|---|---|
| `CHUNK_SIZE` | `800` | 청크 크기 |
| `CHUNK_OVERLAP` | `200` | 청크 오버랩 |
| `LIBREOFFICE_TIMEOUT` | `60` | `.doc` 변환 타임아웃 |
| `LOG_LEVEL` | `INFO` | 로그 레벨 |
| `API_DOCS_URL` | `/docs` | Swagger 경로 |
| `API_REDOC_URL` | `/redoc` | ReDoc 경로 |

---

## 5) 실행

```bash
python -m uvicorn app.converter:app --host 0.0.0.0 --port 8000 --loop asyncio
```

검증:

```bash
curl http://localhost:8000/health
curl http://localhost:1234/api/v1/models
curl http://localhost:8001/v1/models
```