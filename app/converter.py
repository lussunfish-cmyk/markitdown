"""
íŒŒì¼ ë³€í™˜ REST API ì• í”Œë¦¬ì¼€ì´ì…˜.
ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì„ MarkItDown ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import json
import logging
import os
import subprocess
import tempfile
import time
from pathlib import Path
from typing import Optional, List, Dict, Any

from fastapi import FastAPI, File, HTTPException, UploadFile, Form, Query # type: ignore
from fastapi.responses import FileResponse, StreamingResponse, HTMLResponse  # type: ignore
from fastapi.staticfiles import StaticFiles  # type: ignore
from pydantic import BaseModel  # type: ignore

from markitdown import MarkItDown   # type: ignore
from .config import config
from .schemas import (
    ConversionFileResult,
    BatchConversionResult,
    SupportedFormatsResponse,
    BatchJobResponse,
    IndexResponse,
    IndexFileRequest,
    IndexFolderRequest,
    RAGRequest,
    RAGResponse,
    RetrievalResult,
    SearchResult
)

# ============================================================================
# ì„¤ì • ë° ì´ˆê¸°í™”
# ============================================================================


def setup_logging() -> logging.Logger:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê¹…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)


logger = setup_logging()
app = FastAPI(title=config.API_TITLE)

# Static files ë§ˆìš´íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œ)
static_path = Path(__file__).parent / "static"
if static_path.exists():
    app.mount("/static", StaticFiles(directory=str(static_path)), name="static")

# ============================================================================
# ë³´ì¡° í•¨ìˆ˜
# ============================================================================


def is_supported_format(file_path: Path) -> bool:
    """íŒŒì¼ í˜•ì‹ì´ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return file_path.suffix.lower() in config.CONVERSION.SUPPORTED_FORMATS


def get_supported_files(directory: Path) -> list[Path]:
    """ë””ë ‰í† ë¦¬ì—ì„œ ì§€ì›ë˜ëŠ” ëª¨ë“  íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    return [
        f for f in directory.rglob("*")
        if f.is_file() and is_supported_format(f)
    ]


def cleanup_temp_file(file_path: Optional[Path]) -> None:
    """ì„ì‹œ íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤."""
    if file_path and file_path.exists():
        try:
            file_path.unlink()
        except Exception:
            pass

# ============================================================================
# íŒŒì¼ ë³€í™˜ í•¨ìˆ˜
# ============================================================================


def convert_doc_to_docx(doc_path: Path) -> tuple[Optional[Path], str]:
    """
    LibreOfficeë¥¼ ì‚¬ìš©í•˜ì—¬ .doc íŒŒì¼ì„ .docxë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        doc_path: .doc íŒŒì¼ ê²½ë¡œ
        
    Returns:
        (.docx íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None, ì—ëŸ¬ ë©”ì‹œì§€ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)ì˜ íŠœí”Œ
    """
    try:
        result = subprocess.run(
            [
                "libreoffice",
                "--headless",
                "--convert-to", "docx",
                "--outdir", str(doc_path.parent),
                str(doc_path)
            ],
            capture_output=True,
            timeout=config.CONVERSION.LIBREOFFICE_TIMEOUT,
            text=True
        )
        
        docx_path = doc_path.parent / f"{doc_path.stem}.docx"
        
        if result.returncode == 0 and docx_path.exists():
            return docx_path, ""
        else:
            error_output = result.stderr if result.stderr else result.stdout
            return None, f"LibreOffice ë³€í™˜ ì‹¤íŒ¨: {error_output}"
    except subprocess.TimeoutExpired:
        return None, f"LibreOffice ë³€í™˜ íƒ€ì„ì•„ì›ƒ (>{config.CONVERSION.LIBREOFFICE_TIMEOUT}ì´ˆ)"
    except FileNotFoundError:
        return None, "LibreOfficeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
    except Exception as e:
        return None, f"ë³€í™˜ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}"


def extract_markdown(file_path: Path) -> tuple[Optional[str], str]:
    """
    íŒŒì¼ì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        file_path: ë³€í™˜í•  íŒŒì¼ ê²½ë¡œ
        
    Returns:
        (ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ë˜ëŠ” None, ì—ëŸ¬ ë©”ì‹œì§€ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)ì˜ íŠœí”Œ
    """
    try:
        converter = MarkItDown()
        result = converter.convert(str(file_path))
        
        markdown_text = getattr(result, "text_content", None)
        if not markdown_text:
            markdown_text = getattr(result, "text", None)
        
        if not markdown_text:
            return None, "ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ ì‹¤íŒ¨"
        
        # í¼ í”¼ë“œ ë¬¸ì ì œê±°
        markdown_text = markdown_text.replace('\f', '')
        return markdown_text, ""
    except Exception as e:
        return None, f"ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ ì—ëŸ¬: {str(e)}"


def save_markdown(markdown_text: str, output_filename: str) -> tuple[bool, str]:
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        markdown_text: ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        output_filename: ì¶œë ¥ íŒŒì¼ëª…
        
    Returns:
        (ì„±ê³µ ì—¬ë¶€, ì—ëŸ¬ ë©”ì‹œì§€ ë˜ëŠ” ë¹ˆ ë¬¸ìì—´)ì˜ íŠœí”Œ
    """
    try:
        output_path = config.CONVERSION.OUTPUT_DIR / output_filename
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(markdown_text)
        return True, ""
    except Exception as e:
        return False, f"íŒŒì¼ ì €ì¥ ì—ëŸ¬: {str(e)}"


def convert_single_file(
    file_path: Path,
    log_progress: bool = False
) -> tuple[bool, str, str, float]:
    """
    ë‹¨ì¼ íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        file_path: ë³€í™˜í•  íŒŒì¼ ê²½ë¡œ
        log_progress: ì§„í–‰ ìƒí™©ì„ ì½˜ì†”ì— ê¸°ë¡í• ì§€ ì—¬ë¶€
        
    Returns:
        (ì„±ê³µ ì—¬ë¶€, íŒŒì¼ëª…, ë©”ì‹œì§€, ì†Œìš” ì‹œê°„(ì´ˆ))ì˜ íŠœí”Œ
    """
    start_time = time.time()
    
    if log_progress:
        logger.info(f"ğŸ”„ ë³€í™˜ ì¤‘: {file_path.name}")
    
    # íŒŒì¼ í˜•ì‹ì´ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸
    if not is_supported_format(file_path):
        duration = time.time() - start_time
        error_msg = f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {file_path.suffix}"
        if log_progress:
            logger.error(f"âŒ ì‹¤íŒ¨: {file_path.name} - {error_msg} ({duration:.2f}ì´ˆ)")
        return False, file_path.name, error_msg, duration

    actual_file_path = file_path
    temp_converted_docx = None

    try:
        # .docë¥¼ .docxë¡œ ë³€í™˜ í•„ìš” ì‹œ
        if file_path.suffix.lower() == ".doc":
            if log_progress:
                logger.info(f"  ğŸ“„ .docë¥¼ .docxë¡œ ë³€í™˜ ì¤‘...")
            
            temp_converted_docx, error_msg = convert_doc_to_docx(file_path)
            if not temp_converted_docx:
                duration = time.time() - start_time
                full_error = f".docë¥¼ .docxë¡œ ë³€í™˜ ì‹¤íŒ¨: {error_msg}"
                if log_progress:
                    logger.error(f"âŒ ì‹¤íŒ¨: {file_path.name} - {error_msg} ({duration:.2f}ì´ˆ)")
                return False, file_path.name, full_error, duration
            
            actual_file_path = temp_converted_docx

        # ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œ
        markdown_text, extract_error = extract_markdown(actual_file_path)
        if not markdown_text:
            duration = time.time() - start_time
            if log_progress:
                logger.error(f"âŒ ì‹¤íŒ¨: {file_path.name} - {extract_error} ({duration:.2f}ì´ˆ)")
            return False, file_path.name, extract_error, duration

        # ë§ˆí¬ë‹¤ìš´ ì €ì¥
        output_filename = f"{file_path.stem}.md"
        success, save_error = save_markdown(markdown_text, output_filename)
        
        if not success:
            duration = time.time() - start_time
            if log_progress:
                logger.error(f"âŒ ì‹¤íŒ¨: {file_path.name} - {save_error} ({duration:.2f}ì´ˆ)")
            return False, file_path.name, save_error, duration

        duration = time.time() - start_time
        if log_progress:
            logger.info(f"âœ… ì„±ê³µ: {file_path.name} â†’ {output_filename} ({duration:.2f}ì´ˆ)")
        
        return True, output_filename, "ì„±ê³µì ìœ¼ë¡œ ë³€í™˜ë¨", duration

    except Exception as e:
        duration = time.time() - start_time
        error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}"
        if log_progress:
            logger.error(f"âŒ ì‹¤íŒ¨: {file_path.name} - {error_msg} ({duration:.2f}ì´ˆ)")
        return False, file_path.name, error_msg, duration
    
    finally:
        cleanup_temp_file(temp_converted_docx)

# ============================================================================
# ê²°ê³¼ ì²˜ë¦¬
# ============================================================================


def save_result_json(result: dict) -> Path:
    """
    ë³€í™˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        result: ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì €ì¥ëœ JSON íŒŒì¼ ê²½ë¡œ
    """
    result_path = config.CONVERSION.OUTPUT_DIR / config.CONVERSION.RESULT_FILENAME
    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    return result_path


def log_batch_summary(total: int, converted: int, failed: int, duration: float) -> None:
    """ë°°ì¹˜ ë³€í™˜ ìš”ì•½ì„ ê¸°ë¡í•©ë‹ˆë‹¤."""
    logger.info("\n" + "="*60)
    logger.info(f"ğŸ ë°°ì¹˜ ë³€í™˜ ì™„ë£Œ")
    logger.info(f"   ì´ íŒŒì¼: {total}ê°œ")
    logger.info(f"   âœ… ì„±ê³µ: {converted}ê°œ")
    logger.info(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")
    logger.info(f"   â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
    logger.info("="*60)


def create_batch_result(
    total_files: int,
    converted_files: int,
    failed_files: int,
    total_duration: float,
    files: list[dict]
) -> dict:
    """ë°°ì¹˜ ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        "total_files": total_files,
        "converted_files": converted_files,
        "failed_files": failed_files,
        "total_duration": round(total_duration, 2),
        "files": files,
        "message": f"ë°°ì¹˜ ë³€í™˜ ì™„ë£Œ: {converted_files}ê°œ ì„±ê³µ, {failed_files}ê°œ ì‹¤íŒ¨"
    }

# ============================================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================================


@app.post("/convert")
async def convert_file(
    file: UploadFile = File(...),
    auto_index: bool = Form(False)
) -> Dict[str, Any]:
    """
    ì—…ë¡œë“œëœ ë‹¨ì¼ íŒŒì¼ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        file: ë³€í™˜í•  íŒŒì¼
        auto_index: ë³€í™˜ í›„ ìë™ìœ¼ë¡œ ì¸ë±ì‹±í• ì§€ ì—¬ë¶€
    
    Returns:
        ë³€í™˜ ê²°ê³¼ ë° ì¸ë±ì‹± ì •ë³´
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="íŒŒì¼ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤")

    logger.info(f"ğŸ“¥ íŒŒì¼ ìˆ˜ì‹ ë¨: {file.filename} (auto_index={auto_index})")
    
    input_suffix = Path(file.filename).suffix.lower() or ".bin"

    with tempfile.NamedTemporaryFile(delete=False, suffix=input_suffix) as tmp_in:
        tmp_in.write(await file.read())
        input_path = tmp_in.name

    try:
        success, output_filename, msg, duration = convert_single_file(
            Path(input_path),
            log_progress=True
        )
        
        if not success:
            raise HTTPException(status_code=500, detail=msg)
        
        output_path = config.CONVERSION.OUTPUT_DIR / output_filename
        
        if not output_path.exists():
            raise HTTPException(status_code=500, detail="ë³€í™˜ëœ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        result = {
            "filename": output_filename,
            "status": "converted",
            "output_path": str(output_path.relative_to(config.CONVERSION.OUTPUT_DIR)),
            "indexed": False,
            "duration": duration
        }
        
        # auto_indexê°€ Trueë©´ ì¦‰ì‹œ ì¸ë±ì‹±
        if auto_index:
            try:
                from .indexer import DocumentIndexer
                indexer = DocumentIndexer()
                index_result = indexer.index_file(output_path)
                
                result["status"] = "converted_and_indexed"
                result["indexed"] = True
                result["index_info"] = {
                    "document_id": output_path.stem,
                    "chunks": index_result.get("total_chunks", 0)
                }
                logger.info(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {output_filename}")
            except Exception as e:
                logger.error(f"âŒ ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
                result["index_error"] = str(e)
        
        return result
        
    finally:
        try:
            os.remove(input_path)
        except FileNotFoundError:
            pass


@app.post("/convert-folder")
async def convert_folder(auto_index: bool = Form(False)) -> FileResponse:
    """
    ì…ë ¥ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ì§€ì› íŒŒì¼ì„ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        auto_index: ë³€í™˜ í›„ ìë™ìœ¼ë¡œ ì¸ë±ì‹±í• ì§€ ì—¬ë¶€
    
    ë³€í™˜ ê²°ê³¼ ë° í†µê³„ê°€ í¬í•¨ëœ JSON íŒŒì¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    batch_start_time = time.time()
    
    logger.info(f"ğŸ“‚ ë°°ì¹˜ ë³€í™˜ ì‹œì‘ (auto_index={auto_index})")
    
    if not config.CONVERSION.INPUT_DIR.exists():
        raise HTTPException(status_code=400, detail="ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

    files_to_convert = get_supported_files(config.CONVERSION.INPUT_DIR)

    logger.info(f"ğŸ“‚ ë°°ì¹˜ ë³€í™˜ ì‹œì‘: {len(files_to_convert)}ê°œ íŒŒì¼ ë°œê²¬")
    logger.info("="*60)

    # ë¹ˆ ë””ë ‰í† ë¦¬ ì²˜ë¦¬
    if not files_to_convert:
        result = create_batch_result(
            total_files=0,
            converted_files=0,
            failed_files=0,
            total_duration=0,
            files=[]
        )
        result_path = save_result_json(result)
        
        return FileResponse(
            path=str(result_path),
            media_type="application/json",
            filename=config.CONVERSION.RESULT_FILENAME
        )

    # íŒŒì¼ ì²˜ë¦¬
    converted = []
    failed = []
    converted_paths = []  # ì¸ë±ì‹±ìš©

    for idx, file_path in enumerate(sorted(files_to_convert), 1):
        logger.info(f"\n[{idx}/{len(files_to_convert)}]")
        success, output_filename, msg, duration = convert_single_file(
            file_path,
            log_progress=True
        )
        
        result_dict = {
            "input": file_path.name,
            "status": "success" if success else "failed",
            "duration": round(duration, 2)
        }
        
        if success:
            result_dict["output"] = output_filename
            converted.append(result_dict)
            converted_paths.append(config.CONVERSION.OUTPUT_DIR / output_filename)
        else:
            result_dict["reason"] = msg
            failed.append(result_dict)

    total_duration = time.time() - batch_start_time
    all_results = converted + failed

    # auto_indexê°€ Trueë©´ ë³€í™˜ëœ íŒŒì¼ë“¤ ì¸ë±ì‹±
    if auto_index and converted_paths:
        logger.info(f"\nâœ¨ ìë™ ì¸ë±ì‹± ì‹œì‘: {len(converted_paths)}ê°œ íŒŒì¼")
        try:
            from .indexer import DocumentIndexer
            import time as time_module
            
            indexer = DocumentIndexer()
            
            indexed_count = 0
            failed_index_count = 0
            
            for i, md_path in enumerate(converted_paths, 1):
                try:
                    logger.info(f"[{i}/{len(converted_paths)}] ì¸ë±ì‹± ì¤‘: {md_path.name}")
                    result = indexer.index_file(md_path, force_reindex=False)
                    
                    if result.get("status") == "indexed":
                        indexed_count += 1
                        logger.info(f"âœ… ì¸ë±ì‹± ì™„ë£Œ: {md_path.name} ({result.get('chunks', 0)} ì²­í¬)")
                    elif result.get("status") == "skipped":
                        indexed_count += 1
                        logger.debug(f"â­ï¸  ìŠ¤í‚µ: {md_path.name} (ì´ë¯¸ ì¸ë±ì‹±ë¨)")
                    else:
                        failed_index_count += 1
                        logger.warning(f"âš ï¸  ì¸ë±ì‹± ì‹¤íŒ¨: {md_path.name} - {result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì—ëŸ¬')}")
                    
                    # ChromaDB readonly ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì§§ì€ ë”œë ˆì´
                    time_module.sleep(0.1)
                    
                except Exception as e:
                    failed_index_count += 1
                    logger.error(f"âŒ ì¸ë±ì‹± ì—ëŸ¬ ({md_path.name}): {e}")
            
            logger.info(f"\nâœ… ì¸ë±ì‹± ì™„ë£Œ: {indexed_count}ê°œ ì„±ê³µ, {failed_index_count}ê°œ ì‹¤íŒ¨")
        except Exception as e:
            logger.error(f"ì¸ë±ì‹± ì´ˆê¸°í™” ì—ëŸ¬: {e}")

    # ìš”ì•½ ê¸°ë¡
    log_batch_summary(
        len(files_to_convert),
        len(converted),
        len(failed),
        total_duration
    )

    # ê²°ê³¼ ìƒì„± ë° ì €ì¥
    result = create_batch_result(
        total_files=len(files_to_convert),
        converted_files=len(converted),
        failed_files=len(failed),
        total_duration=total_duration,
        files=all_results
    )
    
    result_path = save_result_json(result)
    logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {result_path}")
    
    return FileResponse(
        path=str(result_path),
        media_type="application/json",
        filename=config.CONVERSION.RESULT_FILENAME
    )


@app.get("/", response_class=HTMLResponse)
async def root():
    """í”„ë¡ íŠ¸ì—”ë“œ UIë¥¼ ì œê³µí•©ë‹ˆë‹¤."""
    static_path = Path(__file__).parent / "static" / "index.html"
    if static_path.exists():
        return HTMLResponse(content=static_path.read_text(), status_code=200)
    return HTMLResponse(content="<h1>MarkItDown API</h1><p>Frontend not found</p>", status_code=200)


@app.get("/health")
async def health_check() -> dict:
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤."""
    return {"status": "healthy"}


@app.get("/supported-formats")
async def get_supported_formats() -> SupportedFormatsResponse:
    """ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return SupportedFormatsResponse(
        formats=sorted(list(config.CONVERSION.SUPPORTED_FORMATS)),
        count=len(config.CONVERSION.SUPPORTED_FORMATS)
    )


# ============================================================================
# ë°°ì¹˜ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.post("/convert-batch")
async def convert_batch(
    files: List[UploadFile] = File(...),
    batch_size: int = Form(100),
    auto_index: bool = Form(True)
) -> BatchJobResponse:
    """
    ì—¬ëŸ¬ íŒŒì¼ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬.
    
    Args:
        files: ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡
        batch_size: ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
        auto_index: ë³€í™˜ í›„ ìë™ ì„ë² ë”© ì—¬ë¶€
    
    Returns:
        ë°°ì¹˜ ì‘ì—… ì •ë³´ ë° ìƒíƒœ
    """
    from .batch_manager import get_batch_manager
    from .indexer import DocumentIndexer
    
    logger.info(f"ğŸ“¦ ë°°ì¹˜ ì‘ì—… ì‹œì‘: {len(files)}ê°œ íŒŒì¼, batch_size={batch_size}, auto_index={auto_index}")
    
    # íŒŒì¼ë“¤ì„ ì„ì‹œ ì €ì¥
    temp_files = []
    for file in files:
        if not file.filename:
            continue
        
        input_suffix = Path(file.filename).suffix.lower() or ".bin"
        with tempfile.NamedTemporaryFile(delete=False, suffix=input_suffix, dir="/tmp") as tmp:
            tmp.write(await file.read())
            temp_files.append((Path(tmp.name), file.filename))
    
    # ë°°ì¹˜ ì‘ì—… ìƒì„±
    batch_manager = get_batch_manager()
    batch_id = batch_manager.create_batch(
        files=[name for _, name in temp_files],
        batch_size=batch_size,
        auto_index=auto_index
    )
    
    # ë°°ì¹˜ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
    state = batch_manager.load_state(batch_id)
    indexer = DocumentIndexer() if auto_index else None
    
    for batch_info in state["batches"]:
        batch_num = batch_info["batch_num"]
        
        # ë°°ì¹˜ ì‹œì‘
        batch_manager.update_batch_status(batch_id, batch_num, "processing")
        
        for file_info in batch_info["files"]:
            filename = file_info["filename"]
            
            # í•´ë‹¹ íŒŒì¼ì˜ ì„ì‹œ ê²½ë¡œ ì°¾ê¸°
            temp_path = None
            for tmp_path, orig_name in temp_files:
                if orig_name == filename:
                    temp_path = tmp_path
                    break
            
            if not temp_path:
                batch_manager.update_file_status(
                    batch_id, batch_num, filename,
                    status="failed",
                    error="ì„ì‹œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
                continue
            
            try:
                # ë³€í™˜
                start_time = time.time()
                success, output_filename, msg, duration = convert_single_file(
                    temp_path,
                    log_progress=False
                )
                
                if not success:
                    batch_manager.update_file_status(
                        batch_id, batch_num, filename,
                        status="failed",
                        error=msg,
                        duration=duration
                    )
                    continue
                
                md_path = config.CONVERSION.OUTPUT_DIR / output_filename
                
                # ì„ë² ë”© (ì˜µì…˜)
                indexed = False
                if auto_index and indexer:
                    try:
                        indexer.index_file(md_path)
                        indexed = True
                    except Exception as e:
                        logger.error(f"ì¸ë±ì‹± ì‹¤íŒ¨ ({filename}): {e}")
                
                # ì„±ê³µ ìƒíƒœ ì—…ë°ì´íŠ¸
                batch_manager.update_file_status(
                    batch_id, batch_num, filename,
                    status="completed",
                    converted_path=output_filename,
                    indexed=indexed,
                    duration=duration
                )
                
            except Exception as e:
                # ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
                batch_manager.update_file_status(
                    batch_id, batch_num, filename,
                    status="failed",
                    error=str(e)
                )
        
        # ë°°ì¹˜ ì™„ë£Œ
        batch_manager.update_batch_status(batch_id, batch_num, "completed")
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    for tmp_path, _ in temp_files:
        try:
            tmp_path.unlink()
        except:
            pass
    
    # ìµœì¢… ìƒíƒœ ë°˜í™˜
    final_state = batch_manager.load_state(batch_id)
    return BatchJobResponse(**final_state)


@app.get("/batch/{batch_id}")
async def get_batch_status(batch_id: str) -> BatchJobResponse:
    """
    ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¡°íšŒ.
    
    Args:
        batch_id: ë°°ì¹˜ ì‘ì—… ID
    
    Returns:
        í˜„ì¬ ë°°ì¹˜ ì‘ì—… ìƒíƒœ
    """
    from .batch_manager import get_batch_manager
    
    batch_manager = get_batch_manager()
    
    if not batch_manager.batch_exists(batch_id):
        raise HTTPException(404, "ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    state = batch_manager.load_state(batch_id)
    return BatchJobResponse(**state)


@app.delete("/batch/{batch_id}")
async def delete_batch(batch_id: str) -> Dict[str, str]:
    """
    ë°°ì¹˜ ì‘ì—… ì‚­ì œ.
    
    Args:
        batch_id: ë°°ì¹˜ ì‘ì—… ID
    
    Returns:
        ì‚­ì œ í™•ì¸ ë©”ì‹œì§€
    """
    from .batch_manager import get_batch_manager
    
    batch_manager = get_batch_manager()
    
    if not batch_manager.batch_exists(batch_id):
        raise HTTPException(404, "ë°°ì¹˜ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    batch_manager.delete_batch(batch_id)
    return {"message": f"ë°°ì¹˜ {batch_id} ì‚­ì œ ì™„ë£Œ"}


@app.post("/batch/folder")
async def batch_folder(
    batch_size: int = Query(100, description="ë°°ì¹˜ë‹¹ íŒŒì¼ ìˆ˜"),
    auto_index: bool = Query(True, description="ìë™ ì¸ë±ì‹± ì—¬ë¶€")
) -> BatchJobResponse:
    """
    input í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ë°°ì¹˜ ì²˜ë¦¬ (íŒŒì¼ ì—…ë¡œë“œ ì—†ì´).
    
    Args:
        batch_size: ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜
        auto_index: ë³€í™˜ í›„ ìë™ ì„ë² ë”© ì—¬ë¶€
    
    Returns:
        ë°°ì¹˜ ì‘ì—… ì •ë³´ ë° ìƒíƒœ
    """
    from .batch_manager import get_batch_manager
    from .indexer import DocumentIndexer
    
    if not config.CONVERSION.INPUT_DIR.exists():
        raise HTTPException(400, "ì…ë ¥ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
    
    files_to_convert = get_supported_files(config.CONVERSION.INPUT_DIR)
    
    if not files_to_convert:
        raise HTTPException(400, "ë³€í™˜í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    
    logger.info(f"ğŸ“¦ Input í´ë” ë°°ì¹˜ ì‘ì—… ì‹œì‘: {len(files_to_convert)}ê°œ íŒŒì¼, batch_size={batch_size}, auto_index={auto_index}")
    
    # ë°°ì¹˜ ì‘ì—… ìƒì„±
    batch_manager = get_batch_manager()
    batch_id = batch_manager.create_batch(
        files=[str(f) for f in files_to_convert],
        batch_size=batch_size,
        auto_index=auto_index
    )
    
    # ë°°ì¹˜ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
    state = batch_manager.load_state(batch_id)
    indexer = DocumentIndexer() if auto_index else None
    
    for batch_info in state["batches"]:
        batch_num = batch_info["batch_num"]
        
        # ë°°ì¹˜ ì‹œì‘
        batch_manager.update_batch_status(batch_id, batch_num, "processing")
        
        for file_info in batch_info["files"]:
            filename = file_info["filename"]
            
            # íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            file_path = Path(filename)
            if not file_path.exists():
                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° input ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì°¾ê¸°
                file_path = config.CONVERSION.INPUT_DIR / Path(filename).name
            
            if not file_path.exists():
                batch_manager.update_file_status(
                    batch_id, batch_num, filename,
                    status="failed",
                    error="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
                continue
            
            try:
                # ë³€í™˜
                success, output_filename, msg, duration = convert_single_file(
                    file_path,
                    log_progress=True
                )
                
                if not success:
                    batch_manager.update_file_status(
                        batch_id, batch_num, filename,
                        status="failed",
                        error=msg,
                        duration=duration
                    )
                    continue
                
                md_path = config.CONVERSION.OUTPUT_DIR / output_filename
                
                # ì„ë² ë”© (ì˜µì…˜)
                indexed = False
                if auto_index and indexer:
                    try:
                        indexer.index_file(md_path)
                        indexed = True
                    except Exception as e:
                        logger.error(f"ì¸ë±ì‹± ì‹¤íŒ¨ ({filename}): {e}")
                
                # ì„±ê³µ ìƒíƒœ ì—…ë°ì´íŠ¸
                batch_manager.update_file_status(
                    batch_id, batch_num, filename,
                    status="completed",
                    converted_path=output_filename,
                    indexed=indexed,
                    duration=duration
                )
                
            except Exception as e:
                # ì‹¤íŒ¨ ìƒíƒœ ì—…ë°ì´íŠ¸
                batch_manager.update_file_status(
                    batch_id, batch_num, filename,
                    status="failed",
                    error=str(e)
                )
        
        # ë°°ì¹˜ ì™„ë£Œ
        batch_manager.update_batch_status(batch_id, batch_num, "completed")
    
    # ìµœì¢… ìƒíƒœ ë°˜í™˜
    final_state = batch_manager.load_state(batch_id)
    return BatchJobResponse(**final_state)


@app.get("/batch")
async def list_batches() -> Dict[str, Any]:
    """
    ì €ì¥ëœ ëª¨ë“  ë°°ì¹˜ ì‘ì—… ëª©ë¡ ì¡°íšŒ.
    
    Returns:
        ë°°ì¹˜ ID ëª©ë¡ ë° ìš”ì•½ ì •ë³´
    """
    from .batch_manager import get_batch_manager
    
    batch_manager = get_batch_manager()
    batch_ids = batch_manager.list_batches()
    
    batches_summary = []
    for batch_id in batch_ids:
        try:
            state = batch_manager.load_state(batch_id)
            batches_summary.append({
                "batch_id": batch_id,
                "status": state.get("status"),
                "total_files": state.get("total_files"),
                "progress_percentage": state.get("progress_percentage"),
                "started_at": state.get("started_at")
            })
        except Exception as e:
            logger.error(f"ë°°ì¹˜ {batch_id} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return {
        "total_batches": len(batches_summary),
        "batches": batches_summary
    }


# ============================================================================
# ì¸ë±ì‹± ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.post("/index")
async def index_document(request: IndexFileRequest) -> Dict[str, Any]:
    """
    ë‹¨ì¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì¸ë±ì‹±.
    
    Args:
        request: ì¸ë±ì‹± ìš”ì²­ (file_path, force)
    
    Returns:
        ì¸ë±ì‹± ê²°ê³¼
    """
    from .indexer import DocumentIndexer
    
    indexer = DocumentIndexer()
    full_path = config.CONVERSION.OUTPUT_DIR / request.file_path
    
    if not full_path.exists():
        raise HTTPException(404, f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.file_path}")
    
    try:
        result = indexer.index_file(full_path, force_reindex=request.force)
        return {
            "status": "success",
            "message": f"ì¸ë±ì‹± ì™„ë£Œ: {request.file_path}",
            "result": result
        }
    except Exception as e:
        logger.error(f"ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")


@app.post("/index-folder")
async def index_folder(request: IndexFolderRequest) -> IndexResponse:
    """
    output í´ë”ì˜ ëª¨ë“  MD íŒŒì¼ ì¸ë±ì‹±.
    
    Args:
        request: í´ë” ì¸ë±ì‹± ìš”ì²­ (folder, force)
    
    Returns:
        ì¸ë±ì‹± ê²°ê³¼
    """
    from .indexer import DocumentIndexer
    
    indexer = DocumentIndexer()
    target_dir = config.CONVERSION.OUTPUT_DIR / request.folder
    
    if not target_dir.exists():
        raise HTTPException(404, f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {request.folder}")
    
    try:
        result = indexer.index_directory(target_dir, force_reindex=request.force)
        return IndexResponse(**result.model_dump()) if hasattr(result, 'model_dump') else IndexResponse(**result)
    except Exception as e:
        logger.error(f"í´ë” ì¸ë±ì‹± ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"í´ë” ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")


@app.get("/documents")
async def list_documents() -> Dict[str, Any]:
    """
    ì¸ë±ì‹±ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ.
    
    Returns:
        ë¬¸ì„œ ëª©ë¡ ë° í†µê³„
    """
    from .indexer import DocumentIndexer
    
    try:
        indexer = DocumentIndexer()
        docs = indexer.get_indexed_files()
        
        return {
            "total": len(docs),
            "documents": [d.model_dump() for d in docs]
        }
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


# ============================================================================
# RAG ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@app.post("/query")
async def query(request: RAGRequest) -> RAGResponse:
    """
    RAG ì§ˆì˜ì‘ë‹µ.
    
    Args:
        request: ì§ˆì˜ ìš”ì²­ (query, top_k ë“±)
    
    Returns:
        ë‹µë³€ ë° ì¶œì²˜
    """
    from .rag import RAGPipeline
    
    try:
        rag = RAGPipeline()
        rag_result = rag.query(
            question=request.query,
            top_k=request.top_k,
            include_sources=request.include_sources
        )
        
        # RAGResultë¥¼ RAGResponseë¡œ ë³€í™˜
        response = RAGResponse(
            answer=rag_result.answer,
            sources=[
                RetrievalResult(
                    content=src.get("content", ""),
                    source=src.get("source", ""),
                    chunk_id=src.get("chunk_id", 0),
                    similarity_score=src.get("score", 0.0)
                )
                for src in rag_result.sources
            ],
            model=config.OLLAMA.LLM_MODEL,
            tokens_used=None
        )
        
        # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì¶œì²˜ ë‚´ìš© í™•ì¸
        if response.sources:
            logger.info(f"ì²« ë²ˆì§¸ ì¶œì²˜ - content ê¸¸ì´: {len(response.sources[0].content)}, source: {response.sources[0].source}")
        
        return response
    except Exception as e:
        logger.error(f"RAG ì§ˆì˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"RAG ì§ˆì˜ ì‹¤íŒ¨: {str(e)}")


@app.get("/search")
async def search_documents(
    query: str = Query(..., description="ê²€ìƒ‰ ì¿¼ë¦¬"),
    top_k: int = Query(5, description="ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜")
) -> Dict[str, Any]:
    """
    ë¬¸ì„œ ê²€ìƒ‰ë§Œ (ë‹µë³€ ìƒì„± ì—†ìŒ).
    
    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        top_k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
    
    Returns:
        ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
    """
    from .retriever import get_retriever
    
    try:
        retriever = get_retriever()
        results = retriever.search(query, k=top_k)
        
        return {
            "query": query,
            "total_results": len(results),
            "results": [
                {
                    "id": r.id,
                    "content": r.content,
                    "score": r.score,
                    "metadata": r.metadata
                }
                for r in results
            ]
        }
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(500, f"ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
